Multi layer perceptron (MLP)

For each vector inside the embedding matrix,

Linear (matrix multiplication) + ReLU + Linear



MLP: Original Neural Network



training of this model: encode the information into the high-dimension embedding



it is hard to have one neuron to represent a single "fact"

superposition

model performance scales very well of the size (dimension)

gpt2 (768 dim) - gpt3 (12288 dim) - gpt4 (~16000 dims: http://linkedin.com/pulse/curse-dimensionality-768-dimensional-word-embeddings-llm-wadkar-h5lde/)



sparse autoencoder