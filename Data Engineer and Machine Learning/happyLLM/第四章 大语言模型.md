随着 Transformer 架构的横空出世，NLP 领域逐步进入预训练-微调范式，以 Transformer 为基础的、通过预训练获得强大文本表示能力的预训练语言模型层出不穷

随着2022年底 ChatGPT 再一次刷新 NLP 的能力上限，**大语言模型**（Large Language Model，LLM）开始接替传统的预训练语言模型（Pre-trained Language Model，PLM） 成为 NLP 的主流方向



### LLM 的定义

语言模型的概念，即通过预测下一个 token 任务来训练的 NLP 模型

广义的 LLM 一般覆盖了从**十亿参数**（如 Qwen-1.5B）到**千亿参数**（如 Grok-314B）的所有大型语言模型。只要模型展现出**涌现能力**，即在一系列复杂任务上表现出远超传统预训练模型（如 BERT、T5）的能力与潜力，都可以称之为 LLM

GPT-3 通过 预训练（Pretraining）、监督微调（Supervised Fine-Tuning，SFT）、强化学习与人类反馈（Reinforcement Learning with Human Feedback，RLHF）三阶段训练



### LLM 的能力

#### 涌现能力（Emergent Abilities）

涌现能力是指同样的模型架构与预训练任务下，某些能力在小型模型中不明显，但在大型模型中特别突出

涌现能力的显现就像是模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的量变引起了质变

#### 上下文学习（In-context Learning）

上下文学习是指允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而**无需额外的训练或参数更新**。

通过使用具备上下文学习能力的 LLM，一般范式开始向 Prompt Engineering 也就是调整 Prompt 来激发 LLM 的能力转变。例如，目前绝大部分 NLP 任务，通过**调整 Prompt 或提供 1~5 个自然语言示例，就可以令 GPT-4 达到超过传统 PLM 微调的效果**

#### 指令遵循（Instruction Following）

通过使用自然语言描述的多任务数据进行微调，也就是所谓的 `指令微调` ，LLM 被证明在同样使用指令形式化描述的未见过的任务上表现良好

指令遵循能力使 LLM 可以真正和多个行业结合起来，通过人工智能技术为人类生活的方方面面赋能，从而为人类带来质的改变

#### 逐步推理（Step by Step Reasoning）

LLM 通过采用思维链（Chain-of-Thought，CoT）推理策略，可以利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案

deepseek R1 (深度思考模式)



### LLM 的特点

#### 多语言支持

由于英文高质量语料目前仍是占据大部分，以 GPT-4 为代表的绝大部分模型在英文上具有显著超越中文的能力

#### 长文本处理

LM 大部分采用了旋转位置编码（Rotary Positional Encoding，**RoPE**）（或者同样具有外推能力的 AliBi）作为位置编码，具有一定的长度外推能力

#### 拓展多模态

通过为 LLM 增加额外的参数来进行图像表示，从而利用 LLM 的强大能力打造支持文字、图像双模态的模型

#### 挥之不去的幻觉

幻觉，是指 LLM 根据 Prompt 杜撰生成虚假、错误信息的表现

目前也有很多研究提供了削弱幻觉的一些方法，如 Prompt 里进行限制、通过 RAG（检索增强生成）来指导生成等，但都还只能一定程度减弱幻觉而**无法彻底根除**



## 如何训练一个 LLM

训练一个完整的 LLM 需要经过图1中的三个阶段——Pretrain、SFT 和 RLHF



### Pretrain

目前主流的 LLM 几乎都采用了 Decoder-Only 的类 GPT 架构（LLaMA 架构），它们的预训练任务也都沿承了 GPT 模型的经典预训练任务——因果语言模型（Causal Language Model，CLM）

LLM 往往需要使用更大规模的预训练语料： 根据由 OpenAI 提出的 Scaling Law：C ~ 6ND，其中 C 为计算量，N 为模型参数，D 为训练的 token 数，可以实验得出训练 token 数应该是模型参数的 1.7倍，也就是说 175B 的 GPT-3，需要使用 300B token 进行预训练。而 LLaMA 更是进一步提出，使用 20倍 token 来训练模型能达到效果最优，因此 175B 的 GPT-3，可以使用3.5T token 数据预训练达到最优性能

**百亿级 LLM** 需要 1024张 A100 **训练一个多月**，而**十亿级** LLM 一般也需要 256张 A100 训练**两、三天**，计算资源消耗非常高



分布式训练框架的核心思路是数据并行和模型并行

但是，当 LLM 扩大到上百亿参数，单张 GPU 内存往往就无法存放完整的模型参数。在这种情况下，可以**将模型拆分到多个 GPU 上，每个 GPU 上存放不同的层或不同的部分，从而实现模型并行**。

主流的分布式训练框架包括 Deepspeed、Megatron-LM、ColossalAI 等，其中，Deepspeed 使用面最广



Deepspeed 的核心策略是 ZeRO 和 CPU-offload

ZeRO 是一种显存优化的数据并行方案，其核心思想是优化数据并行时每张卡的显存占用，从而实现对更大规模模型的支持

ZeRO 将模型训练阶段每张卡被占用的显存分为两类：

- 模型状态（Model States），包括模型参数、模型梯度和优化器 Adam 的状态参数。假设模型参数量为 1M，一般来说，在混合精度训练的情况下，该部分需要 16M 的空间进行存储，其中 Adam 状态参数会占据 12M 的存储空间。
- 剩余状态（Residual States），除了模型状态之外的显存占用，包括激活值、各种缓存和显存碎片。

随着分片的参数量不断增加，**每张卡需要占用的显存也不断减少。当然，分片的增加也就意味着训练中通信开销的增加**



为了使训练出的 LLM 能够覆盖尽可能广的知识面，预训练语料需要组织多种来源的数据，并以一定比例进行混合

不同的 LLM 往往会在开源预训练语料基础上，加入部分私有高质量语料，再基于自己实验得到的最佳配比来构造预训练数据集。事实上，**数据配比**向来是预训练 LLM 的“核心秘籍”，不同的配比往往会相当大程度影响最终模型训练出来的性能

预训练数据的质量往往比体量更加重要

预训练数据处理一般包括以下流程：

1. 文档准备。由于海量预训练语料往往是从互联网上获得，一般需要从爬取的网站来获得自然语言文档。文档准备主要包括 URL 过滤（根据网页 URL 过滤掉有害内容）、文档提取（从 HTML 中提取纯文本）、语言选择（确定提取的文本的语种）等。
2. 语料过滤。语料过滤的核心目的是去除低质量、无意义、有毒有害的内容，例如乱码、广告等。语料过滤一般有两种方法：基于模型的方法，即通过高质量语料库训练一个文本分类器进行过滤；基于启发式的方法，一般通过人工定义 web 内容的质量指标，计算语料的指标值来进行过滤。
3. 语料去重。实验表示，大量重复文本会显著影响模型的泛化能力，因此，语料去重即删除训练语料中相似度非常高的文档，也是必不可少的一个步骤。去重一般基于 hash 算法计算数据集内部或跨数据集的文档相似性，将相似性大于指定阈值的文档去除；也可以基于子串在序列级进行精确匹配去重。



### SFT

 SFT（Supervised Fine-Tuning，有监督微调）。所谓有监督微调，其实就是我们在第三章中讲过的预训练-微调中的微调

而面对能力强大的 LLM，我们往往不再是在指定下游任务上构造有监督数据进行微调，而是选择训练模型的“通用指令遵循能力”，也就是一般通过`指令微调`的方式来进行 SFT。SFT 的主要目标是让模型从多种类型、多种风格的指令中获得泛化的**指令遵循能力**

高质量的指令数据集具有较高的获取难度。不同于预训练使用的无监督语料，**SFT 使用的指令数据集是有监督语料**，除去设计广泛、合理的指令外，还需要对指令回复进行人工标注，并保证标注的高质量



模型是否支持多轮对话，与预训练是没有关系的。事实上，模型的多轮对话能力完全来自于 SFT 阶段

如果要使模型支持多轮对话，我们需要在 SFT 时将训练数据构造成多轮对话格式，让模型能够利用之前的知识来生成回答

1. 直接要求模型预测每一轮对话的输出：

   ```tex
    input=<prompt_1><completion_1><prompt_2><completion_2><prompt_3><completion_3>
    output=[MASK]<completion_1>[MASK]<completion_2>[MASK]<completion_3>
   ```



### RLHF

Reinforcement Learning from Human Feedback

相较于在 GPT-3 就已经初见雏形的 SFT，**RLHF 往往被认为是 ChatGPT 相较于 GPT-3 的最核心突破**

从功能上出发，我们可以将 LLM 的训练过程分成预训练与对齐（alignment）两个阶段

而所谓对齐，其实就是让模型**与人类价值观**一致，从而输出人类希望其输出的内容



RLHF 分为两个步骤：训练 RM 和 PPO 训练

RM，Reward Model，即奖励模型。RM 是用于拟合人类偏好，来给 LLM 做出反馈的

RM 本质上是一个文本分类模型，对于一个文本输出一个标量奖励，和文本分类任务中的隐藏层输出非常类似。在具体实现上，RM 也往往就是传统的 LLM 架构（或 BERT 架构）加上一层**分类层**

RM 训练的偏好数据往往是由人工标注的。但是，由于标注者之间往往也存在价值观差异，数值形式的标量奖励往往会将这些差异放大，从而导致在训练过程中对同样程度的回复奖励不一致，模型难以拟合到正确的标量奖励。因此，**我们往往对同一个 completion 下的不同回复进行排名，再将排名转化为奖励**



在完成 RM 训练之后，就可以使用 PPO 算法来进行强化学习训练。PPO，**Proximal Policy Optimization**，近端策略优化算法，是一种经典的 RL 算法



在具体 PPO 训练过程中，会存在四个模型。如图4.5所示，两个 LLM 和两个 RM。两个 LLM 分别是进行微调、参数更新的 **actor model** 和不进行参数更新的 **ref model**，均是从 SFT 之后的 LLM 初始化的。两个 RM 分别是进行参数更新的 **critic model** 和不进行参数更新的 **reward model**，均是从上一步训练的 RM 初始化的。