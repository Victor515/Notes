PLM： pre-training language model



针对 Encoder、Decoder 的特点，引入 ELMo 的预训练思路，开始出现不同的、对 Transformer 进行优化的思路:

Google 仅选择了 **Encoder** 层，通过将 Encoder 层进行堆叠，再提出不同的预训练任务-掩码语言模型（**Masked Language Model**，MLM），打造了一统自然语言理解（Natural Language Understanding，NLU）任务的代表模型——BERT。

而 OpenAI 则选择了 **Decoder** 层，使用原有的语言模型（Language Model，LM）任务，通过不断增加模型参数和预训练语料，打造了在 NLG（**Natural Language Generation**，自然语言生成）任务上优势明显的 GPT 系列模型



### BERT

 Bidirectional Encoder Representations from Transformers，是由 Google 团队在 2018年发布的预训练语言模型

BERT 是自然语言处理的一个阶段性成果，标志着各种自然语言处理任务的重大进展以及**预训练模型**的统治地位建立，一直到 LLM 的诞生，NLP 领域的主导地位才从 BERT 系模型进行迁移

#### 思想沿承

- BERT 正沿承了 Transformer 的思想，在 Transformer 的模型基座上进行优化，通过将 Encoder 结构进行堆叠，扩大模型参数，打造了在 NLU 任务上独居天分的模型架构
- 预训练+微调范式。2018年，ELMo 的诞生标志着预训练+微调范式的诞生 （基于双向 LSTM 架构）。BERT引入更适合文本理解、能捕捉深层双向语义关系的预训练任务 MLM （模型架构调整为 Transformer），将预训练-微调范式推向了高潮。

#### BERT模型架构——Encoder Only

- Tokenizer
- Embedding
- Encoder
- Prediction_heads

BERT 将相对位置编码融合在了注意力机制中，将相对位置编码同样视为可训练的权重参数

对 BERT 而言能处理的最大上下文长度是 512 个 token

#### 预训练任务——MLM + NSP

NSP（Next Sentence Prediction，下一句预测）

思想：通过将**预训练和微调**分离，完成一次预训练的模型可以仅通过微调应用在几乎所有下游任务上，只要微调的成本较低，即使预训练成本是之前的数倍甚至数十倍，模型仍然有更大的应用价值

Why LM (language modeling): 预训练数据的核心要求即是需要极大的数据规模（数亿 token）。预训练数据一定是从**无监督的语料**中获取。LM 使用上文预测下文的方式可以直接应用到任何文本中，对于任意文本，我们只需要将下文遮蔽将上文输入模型要求其预测就可以实现 LM 训练，因此互联网上所有文本语料都可以被用于预训练。

Why MLM (masked language modeling): LM 预训练任务的一大缺陷在于，其直接拟合从左到右的语义关系，但忽略了双向的语义关系。相较于模拟人类写作的 LM，MLM 模拟的是“完形填空”： 在一个文本序列中随机遮蔽部分 token，然后将所有未被遮蔽的 token 输入模型，要求模型根据输入预测被遮蔽的 token

（？）在具体进行 MLM 训练时，会随机选择训练语料中 15% 的 token 用于遮蔽。但是这 15% 的 token 并非全部被遮蔽为 `<MASK>`。有 80% 的概率被遮蔽，10% 的概率被替换为任意一个 token，还有 10% 的概率保持不变。



除去 MLM，BERT 还提出了另外一个预训练任务——NSP，即下一个句子预测

NSP 任务的核心思路是要求模型判断一个句对的两个句子是否是连续的上下文

由于 NSP 的正样本可以从无监督语料中随机抽取任意连续的句子，而负样本可以对句子打乱后随机抽取（只需要保证不要抽取到原本就连续的句子就行），因此也可以具有几乎无限量的训练数据。



训练规模

BERT 使用了 800M 的 BooksCorpus 语料和 2500M 的英文维基百科语料，90% 的数据使用 128 的上下文长度训练，剩余 10% 的数据使用 512 作为上下文长度进行预训练，总共约训练了 **3.3B** token。其训练的超参数也是值得关注的，BERT 的训练语料共有 13GB 大小，其在 256 的 batch size 上训练了 1M 步（40 个 Epoch）



### RoBERTa

why? 但是，13GB 的预训练数据是否让 BERT 达到了充分的拟合呢？如果我们使用更多预训练语料，是否可以进一步增强模型性能？更多的，BERT 所选用的预训练任务、训练超参数是否是最优的？RoBERTa 应运而生。

#### 优化一：去掉 NSP 预训练任务

#### 优化二：更大规模的预训练数据和预训练步长

#### 优化三：更大的 bpe 词表

BPE，即 Byte Pair Encoding，字节对编码



### ALBERT

是否能够减小模型参数保持模型能力的角度展开了探究

#### 优化一：将 Embedding 参数进行分解

#### 优化二：跨层进行参数共享

在具体实现上，其实就是 ALBERT 仅初始化了一个 Encoder 层

#### 优化三：提出 SOP 预训练任务

在传统的 NSP 任务中，正例是由两个连续句子组成的句对，而负例则是从任意两篇文档中抽取出的句对，模型可以较容易地判断正负例，并不能很好地学习深度语义。

而 SOP 任务提出的改进是，正例同样由两个连续句子组成，**但负例是将这两个的顺序反过来**。也就是说，模型不仅要拟合两个句子之间的关系，更要学习其顺序关系，这样就大大提升了预训练的难度。





## Encoder-Decoder PLM

### T5

Text-To-Text Transfer Transformer

 Encoder-Decoder 结构的模型

编码器用于处理输入文本，解码器用于生成输出文本。编码器和解码器之间通过注意力机制进行信息交互，从而实现输入文本到输出文本的转换

T5 的 Self-Attention 机制和 BERT 的 Attention 机制是一样的，都是基于 Self-Attention 机制设计的



#### 预训练任务

T5模型的预训练任务是 MLM，也称为BERT-style目标

预训练数据集: T5 使用了自己创建的大规模数据集"Colossal Clean Crawled Corpus"(C4)，该数据集从Common Crawl中提取了大量干净的英语文本。C4数据集经过了一定的清洗，去除了无意义的文本、重复文本等

预训练到微调的转换: 预训练完成后，T5模型会在下游任务上进行微调。**微调时，模型在任务特定的数据集上进行训练，并根据任务调整解码策略**。



#### 大一统思想

其设计理念是将所有不同类型的NLP任务（如文本分类、翻译、文本生成、问答等）转换为一个统一的格式：**输入和输出都是纯文本**

对于不同的NLP任务，每次输入前都会加上一个**任务描述前缀**，明确指定当前任务的类型。这不仅帮助模型在预训练阶段学习到不同任务之间的通用特征，也便于在微调阶段迅速适应具体任务:

- 任务前缀可以是“summarize: ”用于摘要任务，或“translate English to German: ”用于翻译任务。



## Decoder-Only PLM

Decoder-Only 就是目前大火的 LLM 的基础架构:

ChatGPT，正是 Decoder-Only 系列的代表模型 GPT 系列模型的大成之作。而目前作为开源 LLM 基本架构的 LLaMA 模型，也正是在 GPT 的模型架构基础上优化发展而来。



### GPT

Generative Pre-Training Language Model

Decoder-Only 的模型结构往往更适合于文本生成任务



#### 预训练任务——CLM

Casual Language Model，下简称 CLM

N-gram 语言模型是基于前 N 个 token 来预测下一个 token，CLM 则是基于一个自然语言序列的前面所有 token 来预测下一个 token

CLM 是更直接的预训练任务，其天生和人类书写自然语言文本的习惯相契合，也和下游任务直接匹配



#### GPT 系列模型的发展

GPT-2 的另一个重大突破是以 zero-shot（零样本学习）为主要目标

zero-shot 的思路自然是比预训练-微调范式更进一步、更高效的自然语言范式

在大模型时代，zero-shot 及其延伸出的 few-shot（少样本学习）才开始逐渐成为主流。



GPT3 参数量 175B

在预训练数据上，则是分别从 CC、WebText、维基百科等大型语料集中采样，共采样了 45T、清洗后 570GB 的数据

GPT-3 需要在 1024张 A100（80GB 显存）的分布式训练集群上训练 1个月

few-shot 一般会在 prompt（也就是模型的输入）中增加 3~5个示例，来帮助模型理解

```
zero-shot：请你判断‘这真是一个绝佳的机会’的情感是正向还是负向，如果是正向，输出1；否则输出0

few-shot：请你判断‘这真是一个绝佳的机会’的情感是正向还是负向，如果是正向，输出1；否则输出0。你可以参考以下示例来判断：‘你的表现非常好’——1；‘太糟糕了’——0；‘真是一个好主意’——1。
```

few-shot 也被称为上下文学习（**In-context Learning**），即让模型从提供的上下文中的示例里学习问题的解决方法。

如果对于绝大部分任务都可以通过人为构造 3~5个示例就能让模型解决，**其效率将远高于传统的预训练-微调范式**，意味着 NLP 的进一步落地应用成为可能

通过引入预训练-**指令微调**-**人类反馈强化学习**的三阶段训练，OpenAI 发布了跨时代的 **ChatGPT**



###  LLaMA

#### 模型架构——Decoder Only

#### LLaMA模型的发展历程

**LLaMA-1 系列**

- Meta于2023年2月发布了LLaMA-1，包括7B、13B、30B和65B四个参数量版本。
- 这些模型在超过1T token的语料上进行了预训练，其中最大的65B参数模型在2,048张A100 80G GPU上训练了近21天

**LLaMA-2 系列**：

- 2023年7月，Meta发布了LLaMA-2，包含7B、13B、34B和70B四个参数量版本，除了34B模型外，其他均已开源。
- 引入了分组查询注意力机制（Grouped-Query Attention, GQA）

**LLaMA-3 系列**：

- 2024年4月，Meta发布了LLaMA-3，包括8B和70B两个参数量版本，同时透露400B的LLaMA-3还在训练中。
- LLaMA-3支持**8K长文本**，并采用了编码效率更高的tokenizer，词表大小为128K



### GLM

GLM 的核心创新点主要在于其提出的 GLM（General Language Model，通用语言模型）任务，这也是 GLM 的名字由来

GLM 通过优化一个自回归空白填充任务来实现 MLM 与 CLM 思想的结合。其核心思想是，对于一个输入序列，会类似于 MLM 一样进行随机的掩码，但遮蔽的不是和 MLM 一样的单个 token，而是每次遮蔽一连串 token；模型在学习时，既需要使用遮蔽部分的上下文预测遮蔽部分，在遮蔽部分内部又需要以 CLM 的方式完成被遮蔽的 tokens 的预测